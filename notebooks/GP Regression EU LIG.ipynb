{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().parents[0]\n",
    "import sys\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from bahamas_lig.utils import *\n",
    "import importlib\n",
    "importlib.reload(sys.modules['bahamas_lig'])\n",
    "\n",
    "model_dir = PROJECT_ROOT / \"model_outputs/\"\n",
    "inference_dir = PROJECT_ROOT / \"model_outputs/\"\n",
    "data_dir = PROJECT_ROOT / \"data/\"\n",
    "year = '2023'\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox, Label\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as gaussian\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import chi2\n",
    "import os\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from theano import shared\n",
    "from pymc3.distributions.dist_math import SplineWrapper\n",
    "from scipy.interpolate import interp1d, UnivariateSpline\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"Barnett_EuropeLIG_SuppData_April2023_V2.2.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following javascript cell runs bottom the cell of notebook to define some helper functions and force refresh the IPython displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.execute_cells([-2,-1,-6,-5,-4])\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript \n",
    "Jupyter.notebook.execute_cells([-2,-1,-6,-5,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select models to inspect:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23afa99441c94854ac35bc3d15057017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Lithosphere'), SelectMultiple(layout=Layout(height='250px', width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076e8dd2395c4db8825f484a8a54a78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Plot weighted inference for selected', layout=Layout(width='33%'), style=Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Select models to inspect:\")\n",
    "display(h_box)\n",
    "display(HBox(buttons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d887381b2a423085e6de6052021aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a56e5ab8ad48378193e41fad1b5d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(filtered_df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and widget definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     0,
     42
    ]
   },
   "outputs": [],
   "source": [
    "def get_model_status(inference_dir,model_dir):\n",
    "    model_posterior_dir = str(inference_dir)+'/'+str(f'arviz_traces_{year}')\n",
    "    model_posterior_list=[o[:-3] for o in os.listdir(model_posterior_dir) if '.nc' in o]\n",
    "\n",
    "    model_predict_dir = str(inference_dir)+'/'+str(f'pymc3_post_predict_{year}')\n",
    "    model_predict_list=[o[:-4] for o in os.listdir(model_predict_dir) if '.pkl' in o]\n",
    "\n",
    "    model_files_dir = str(model_dir)\n",
    "    model_files_raw=[o for o in os.listdir(model_files_dir) if '.dat' in o]\n",
    "    unique_models=list(set(['_'.join(a.split('_')[:-1]) for a in model_files_raw]))\n",
    "\n",
    "\n",
    "    model_weights = pd.read_csv(str(inference_dir)+'/'+str('model_weights/model_weights.csv'),index_col=0)\n",
    "\n",
    "    models={}\n",
    "    for u in unique_models:\n",
    "        if '_new' in u:\n",
    "            u_proc=u.replace('_new','')\n",
    "        else:\n",
    "            u_proc = u\n",
    "        models[u]={}\n",
    "        models[u]['Lithosphere']= int(u_proc.split('output')[1].split('.dat')[0].split('Cp')[0])\n",
    "        models[u]['UMV']= int(u_proc.split('output')[1].split('.dat')[0].split('Cp')[1][0])\n",
    "        models[u]['LMV']= int(u_proc.split('output')[1].split('.dat')[0].split('Cp')[1][1:].split('_')[0])\n",
    "        models[u]['ice_history']= u_proc.split('output')[1].split('.dat')[0].split('_')[1]\n",
    "        models[u]['esl_curve']= u_proc.split('output')[1].split('.dat')[0].split('Wael_')[1][0]\n",
    "        if any([u.split('output')[1] in mpl for mpl in model_posterior_list]):\n",
    "            models[u]['posterior_trace']= True\n",
    "        else:\n",
    "            models[u]['posterior_trace']= False\n",
    "        if any([u.split('output')[1] in mpl for mpl in model_predict_list]):\n",
    "            models[u]['posterior_predict']= True\n",
    "        else:\n",
    "            models[u]['posterior_predict']= False\n",
    "        if u in list(model_weights.index):\n",
    "            models[u]['weight']=model_weights.loc[u]['weight']\n",
    "        else:\n",
    "            models[u]['weight']=0\n",
    "\n",
    "    models_df = pd.DataFrame.from_dict(models).T\n",
    "    return models_df  \n",
    "  \n",
    "def inference_model_real_data(data, z_functions, model_name, init=\"adapt_full\", target_accept=0.8, \n",
    "                              keys = [\"coral\", \"highstand\"], cores=4, chains=4, tune=1000, draws=1000):\n",
    "    X_new = np.linspace(115, 130, 200)[:, np.newaxis]\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        ELEVATION = shared(data[\"elevation\"].values)\n",
    "        ELEVATION_U = shared(data[\"elevation_uncertainty\"].values)\n",
    "\n",
    "        age_sd = {}\n",
    "        age = {}\n",
    "        \n",
    "        for samp in data.index():\n",
    "            key=data.loc[samp,'type']\n",
    "            \n",
    "            if (key == \"coral\" or key == \"index\" or key == \"limiting\"):\n",
    "                BoundedNormal = pm.Bound(pm.Normal, lower=117, upper=128)\n",
    "                age[samp] = BoundedNormal(str(samp + \"_age\"), mu=data.loc[samp,'age'], \n",
    "                                          sd=data.loc[samp,'age_uncertainty'], shape=(1))\n",
    "            else:\n",
    "                print(\"data type not implemented or key error, check dataframe\")\n",
    "             \n",
    "            #         for key in keys:\n",
    "#             type_filter = data[\"type\"] == key\n",
    "#             AGE = data[type_filter][\"age\"].values\n",
    "#             AGE_U = data[type_filter][\"age_uncertainty\"].values\n",
    "#             IDS = data[type_filter][\"ID\"].values\n",
    "#             N = data[type_filter][\"age\"].size\n",
    "            \n",
    "#             # age errors by data type\n",
    "#             if (key == \"coral\" or key == \"index\"): #normal age errors\n",
    "#                 BoundedNormal = pm.Bound(pm.Normal, lower=117, upper=128)\n",
    "#                 age[key] = BoundedNormal(str(key + \"_age\"), mu=shared(AGE), sd=shared(AGE_U), shape=(N))\n",
    "#             elif key == \"limiting\":\n",
    "#                 for ID in IDS:\n",
    "                    \n",
    "#                 BoundedNormal = pm.Bound(pm.Normal, lower=117, upper=128)\n",
    "#                 age[key] = BoundedNormal(str(key + \"_age\"), mu=shared(AGE), sd=shared(AGE_U), shape=(N))\n",
    "#             elif key == \"highstand\":\n",
    "#                 age_sd[key] = pm.Wald(str(key + \"_age_sd\"), mu=2, lam=5, shape=(N))\n",
    "#                 age[key] = pm.Deterministic(\n",
    "#                     str(key + \"_age\"), shared(AGE)-1 + age_sd[key]\n",
    "#                 )  # reshaped to improve Hamiltonian Monte Carlo\n",
    "#             else:\n",
    "#                 print(\"data type not implemented or key error, check dataframe\")\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    ## Gaussian Process Kernels\n",
    "        gp_ls = pm.Wald(\"gp_ls\", mu=2, lam=5, shape=1)\n",
    "        gp_var = pm.Normal(\"gp_var\", mu=0, sd=500, shape=1)\n",
    "        m_gmsl = pm.Normal(\"m_gmsl\", 0, 200)\n",
    "        mean_fun = pm.gp.mean.Constant(m_gmsl)\n",
    "        \n",
    "\n",
    "        cov1 = gp_var[0] * pm.gp.cov.ExpQuad(1, gp_ls[0])\n",
    "        cov2 = pm.gp.cov.WhiteNoise(.05) #this noise helps avoid cholesky decomp failures\n",
    "      \n",
    "        gp = pm.gp.Marginal(mean_func=mean_fun,cov_func=cov1+cov2)\n",
    "        \n",
    "        ## collect ages from all types of data\n",
    "        ages = [age[x] for x in data.index()]\n",
    "        ages = pm.Deterministic(\"ages\", tt.concatenate(ages))\n",
    "\n",
    "        ## interpolation of simulated age for GIA correction\n",
    "        N = data[\"age\"].size\n",
    "        GIA = tt.zeros(N, dtype=\"float64\")\n",
    "        for i in range(N):\n",
    "            GIA = tt.set_subtensor(GIA[i], SplineWrapper(z_functions[i])(ages[i]))\n",
    "        gia_collect = pm.Deterministic(\n",
    "            \"GIA\", GIA\n",
    "        )  # samples of GIA model RSL (includes Wael)\n",
    "\n",
    "        ## add water depth to GIA by data type\n",
    "        water_depth_sd = {}\n",
    "        water_depth = {}\n",
    "        for samp in data.index():\n",
    "            key=data.loc[samp,'type']\n",
    "\n",
    "            if key == \"limiting\":\n",
    "                mean = 2\n",
    "                lam=5 \n",
    "  \n",
    "                water_depth[samp] = pm.Wald(\n",
    "                    str(samp + \"_water_depth\"), mu=mean, lam=lam, shape=(1)\n",
    "                )\n",
    "                water_depth[samp]=-1*(water_depth[samp]-1.15) #negative to make terrestrial, 1.15 sets max_like at 0\n",
    "\n",
    "            elif (key == \"highstand\" or key == \"index\"): #no added water depth\n",
    "                water_depth[samp] = pm.Deterministic(\n",
    "                    str(key + \"_water_depth\"), 0)\n",
    "                \n",
    "            else:\n",
    "                print(\"data type not implemented or key error, check dataframe\")\n",
    "\n",
    "        ## long term subsidence\n",
    "        N=data[\"elevation\"].values.size\n",
    "        uplift = pm.Normal(\"uplift_master\", 0, 1)\n",
    "        uplift = np.ones(N)*uplift * shared(data[\"uplift_rate (std)\"].values) + shared(data[\"uplift_rate (per ky)\"].values)\n",
    "        uplift = uplift*ages.flatten() ##had /1000 here for last iteration\n",
    "        uplift_for_each = pm.Deterministic(\"uplift\",uplift)\n",
    "        ## collect all through concat\n",
    "        water_depths = [water_depth[x] for x in data.index()]\n",
    "        water_depths = pm.Deterministic(\"water_depths\", tt.concatenate(water_depths))\n",
    "\n",
    "        # master equation:\n",
    "        # GMSL = Elevation observation +/- elevation uncertainty +/- water depth - GIA + SUBSIDENCE\n",
    "        # keep in mind its solving for change in GMSL from the GMSL used in GIA model\n",
    "\n",
    "        elevations_sd = pm.Normal(\"elev_sd\", 0, 1, shape=(data['age'].size))\n",
    "        elevations = pm.Deterministic(\"elev\", ELEVATION + elevations_sd * ELEVATION_U)\n",
    "#         elevations = pm.Deterministic(\"elev\", ELEVATION)\n",
    "        \n",
    "        gmsl_points = pm.Deterministic(\n",
    "            \"gmsl_points\", elevations + water_depths - GIA.flatten() - uplift #+subsidence\n",
    "        )\n",
    "        \n",
    "        noise = pm.HalfCauchy(\"noise\", beta=5)\n",
    "        \n",
    "        \n",
    "        gmsl_inference = gp.marginal_likelihood(\n",
    "            \"gmsl\",\n",
    "            X=ages[:, np.newaxis],\n",
    "            y=gmsl_points.flatten(),\n",
    "            shape=((N),),\n",
    "            noise=pm.gp.cov.WhiteNoise(sigma=noise),\n",
    "#             Xu=Xu[:, np.newaxis],\n",
    "        )  # GMSL deviation from Wael (esl)\n",
    "        \n",
    "        az_trace = pm.sample(tune=tune,draws=draws,\n",
    "                init=init, progressbar=True, cores=cores, target_accept=target_accept, chains=chains, \n",
    "                          return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        f_pred = gp.conditional(\n",
    "            \"f_pred\", X_new, pred_noise=False\n",
    "        )  \n",
    "        \n",
    "        model_posterior_dir = str(model_dir)+'/'+str(f'arviz_traces_{year}/')\n",
    "        model_predict_dir = str(model_dir)+'/'+str(f'pymc3_post_predict_{year}/')\n",
    "        \n",
    "#         return model_trace\n",
    "        try:\n",
    "            pred_samples = pm.fast_sample_posterior_predictive(\n",
    "            az_trace, var_names=['f_pred'],samples=1000,\n",
    "            )\n",
    "#             az_trace=az.from_pymc3(trace)\n",
    "            az_trace.to_netcdf(model_posterior_dir+model_name+'.nc',groups=[\"posterior\",\"log_likelihood\",\"sample_stats\"])\n",
    "            with open(model_predict_dir+model_name+'.pkl', \"wb\") as buff:\n",
    "                pickle.dump(pred_samples, buff)\n",
    "            print('Success')\n",
    "            time.sleep(1)\n",
    "                \n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Sampling failed, no output saved')\n",
    "            time.sleep(1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": [
     0,
     31,
     43,
     68,
     84,
     108,
     120,
     138,
     187,
     205,
     278,
     293,
     380,
     417,
     429,
     446,
     516,
     534,
     626
    ]
   },
   "outputs": [],
   "source": [
    " def load_model(name):\n",
    "    output_dir = 'output_glac_w_ice6g/'\n",
    "\n",
    "    lats = pd.read_csv(model_dir / \"lats\", delimiter=\",\", header=None)\n",
    "    lons = pd.read_csv(model_dir / \"lons\", delimiter=\",\", header=None)\n",
    "    directory = model_dir / output_dir\n",
    "    age = np.arange(115, 131, 1)\n",
    "\n",
    "    extent = [0, 1, 0, 1]\n",
    "    model_dims = [\n",
    "        np.min(lons.values),\n",
    "        np.max(lons.values),\n",
    "        np.min(lats.values),\n",
    "        np.max(lats.values),]\n",
    "\n",
    "    files = np.sort(os.listdir(directory))\n",
    "    files = [f for f in files if 'output' in f]  # ignores non output\n",
    "    \n",
    "    files = [f for f in files if name in f]\n",
    "\n",
    "    rsl = []\n",
    "    age=[]\n",
    "    for i in range(0, len(files)):\n",
    "        rsl.append(\n",
    "        pd.read_csv(\n",
    "            str(directory) + \"/\" + files[i], delimiter=\",\", header=None\n",
    "        ).values\n",
    "        )\n",
    "        age.append(float(files[i].split('ka')[0].split('_')[-1]))\n",
    "    return rsl, age, model_dims\n",
    "\n",
    "def interpolation_functions(LAT, LON, GIA_MODEL, age, model_dims):\n",
    "    island_Zs = [\n",
    "        [lookup_z(lat, lon, m, model_dims) for lat, lon in zip(LAT, LON)]\n",
    "        for m in GIA_MODEL\n",
    "    ]\n",
    "    island_Zs = np.array(island_Zs)\n",
    "    Zfuns = []\n",
    "    for k in range(island_Zs.shape[1]):\n",
    "        rsl_function = UnivariateSpline(age, island_Zs[:, k], k=1, ext=3, s=0)\n",
    "        Zfuns.append(rsl_function)  ## 3 returns boundary value at extrapolation\n",
    "    return Zfuns\n",
    "\n",
    "def lookup_z(lat, lon, model, model_dims):\n",
    "    \"\"\"\n",
    "    Returns the RSL prediction at a specific lat, lon, on a specific GIA model timeslice.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat: Latitude value\n",
    "    lon: Longitude value\n",
    "    model: A 2d matrix from a GIA model output representing a single timeslice.\n",
    "    model_dims: The real word lat/lon dimensions of the model. [left, right, top, bottom]\n",
    "    Returns\n",
    "    -------\n",
    "    The model RSL prediction nearest the lat, lon pair.\n",
    "    \"\"\"\n",
    "    lat_len = model.shape[0]\n",
    "    lon_len = model.shape[1]\n",
    "    lon_list = np.linspace(model_dims[0], model_dims[1], lon_len)\n",
    "    lat_list = np.linspace(model_dims[3], model_dims[2], lat_len)\n",
    "    lon_id = np.argmin(\n",
    "        np.abs(np.linspace(model_dims[0], model_dims[1], lon_len) - (lon))\n",
    "    )\n",
    "    lat_id = np.argmin(\n",
    "        np.abs(np.linspace(model_dims[3], model_dims[2], lat_len) - (lat))\n",
    "    )\n",
    "    return model[lat_id, lon_id]\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"\n",
    "    Custom load command for pickle objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str or path to file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Object saved through pickle.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n",
    "    \n",
    "def load_data():\n",
    "\n",
    "    df=pd.read_excel(data_dir/f'raw/{data_file}',header=1)\n",
    "    data = {}\n",
    "    filtered = df[df['Indicative \\nRange (m)']!='(limiting)']\n",
    "    data[\"lon\"] = filtered['Longitude'].dropna().values\n",
    "    data[\"lat\"] = filtered['Latitude'].dropna().values\n",
    "    data[\"age\"] = filtered['Age\\n(ka BP)'].dropna().values\n",
    "    data[\"age_uncertainty\"] = filtered['Age Error\\n(1σ, ka)'].dropna().values\n",
    "    data[\"elevation\"] = filtered['Relative Sea \\nLevel (m)'].dropna().values\n",
    "    data[\"elevation_uncertainty\"] = filtered['RSL Error \\n(1σ, m)'].dropna().values\n",
    "    data[\"type\"] = ['index' for l in filtered['Indicative \\nRange (m)'].dropna()]\n",
    "    data[\"lower_limit\"] = filtered['RSL (m)\\n(limiting min)'].dropna().values\n",
    "    data[\"region\"] = filtered['Region'].dropna().values\n",
    "    #     keys = [mapping_uplift[i] for i in data['region']]\n",
    "    data[\"uplift_rate (per ky)\"] = filtered['uplift rate *** (mean, m/kyr)'].dropna().values\n",
    "    data[\"uplift_rate (per ky)\"] = [a for a in data[\"uplift_rate (per ky)\"] if type(a) == type(2.3)]\n",
    "    data[\"uplift_rate (std)\"] = filtered['uplift rate *** (1σ, m/kyr)'].dropna().values\n",
    "\n",
    "    data=pd.DataFrame.from_dict(data)\n",
    "    # data['elevation'][data['elevation']=='(limiting)']=filtered['RSL (limiting)'].dropna().values\n",
    "    data['elevation']=pd.to_numeric(data['elevation'])\n",
    "    return data.copy()\n",
    "\n",
    "def clear_and_run(click):\n",
    "    samp=samples_slider.value\n",
    "    accept=accept_slider.value\n",
    "    output_simulation.clear_output()\n",
    "    with output_simulation:     \n",
    "        to_run = list(models_df.query(\n",
    "                    f\"posterior_trace == {list(widge_post.value)} & posterior_predict == {list(widge_predict.value)} & Lithosphere == {list(widge_lith.value)} & UMV == {list(widge_umv.value)} & LMV == {list(widge_lmv.value)} & `ice_history` == {list(widge_ice.value)} & esl_curve == {list(widge_gmsl.value)}\"\n",
    "                ).index)\n",
    "    \n",
    "        backup(to_run)\n",
    "        run_inferences(to_run,samp,accept)\n",
    "        \n",
    "def plot_on_click(click):\n",
    "    output_simulation.clear_output(wait=True)\n",
    "    with output_simulation: \n",
    "        filtdb=models_df.query(\n",
    "                    f\"posterior_trace == {list(widge_post.value)} & posterior_predict == {list(widge_predict.value)} & Lithosphere == {list(widge_lith.value)} & UMV == {list(widge_umv.value)} & LMV == {list(widge_lmv.value)} & `ice_history` == {list(widge_ice.value)} & esl_curve == {list(widge_gmsl.value)}\")\n",
    "                \n",
    "        to_run = list(filtdb.index)\n",
    "        \n",
    "        if any(filtdb['posterior_trace']==False):\n",
    "            print('Selected models have no traces, please run inference and generate weights..')\n",
    "            return None\n",
    "            \n",
    "        fig=weighted_inference_plot(to_run)\n",
    "#         fig=plt.figure()\n",
    "#         plt.plot(np.random.normal(0,1,100))\n",
    "        plt.show()\n",
    "    return\n",
    "    \n",
    "def backup(to_run):\n",
    "\n",
    "        for f in to_run:\n",
    "            try:\n",
    "                model_posterior_dir = str(model_dir)+'/'+str(f'arviz_traces_{year}')\n",
    "                os.rename(model_posterior_dir+'/'+f+'.nc', \n",
    "                          model_posterior_dir+'_backup/'+f+'.nc')\n",
    "                model_predict_dir = str(model_dir)+'/'+str(f'pymc3_post_predict_{year}')\n",
    "                os.rename(model_predict_dir+'/'+f+'.pkl', \n",
    "                          model_predict_dir+'_backup/'+f+'.pkl')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "#     model_predict_dir = str(model_dir)+'/'+str('pymc3_post_predict_2021')\n",
    "#     model_predict_list=[o[:-4] for o in os.listdir(model_predict_dir) if '.pkl' in o]\n",
    "    \n",
    "\n",
    "# def run_inferences(to_run,samp,accept):\n",
    "\n",
    "#     data2 = load_data()\n",
    "#     N = data2[\"elevation\"].size\n",
    "#     data2['age_uncertainty']=data2['age_uncertainty']\n",
    "#     data2=data2.sort_values(['type'])\n",
    "#     keys = list(data2['type'].unique())\n",
    "\n",
    "#     count=1\n",
    "\n",
    "#     for m in to_run:\n",
    "#         clear_output(wait=True)\n",
    "#         print(\"running simulation number \" + str(count) + \" of \" + str(len(to_run)))\n",
    "#         print(\"running model: \" + m)\n",
    "#         count+=1\n",
    "\n",
    "#         GIA_MODEL, age, model_dims = load_model(m)\n",
    "\n",
    "#         z_functions = interpolation_functions(data2[\"lat\"], data2[\"lon\"], GIA_MODEL, age, model_dims)\n",
    "\n",
    "#         inference_model_real_data(\n",
    "#         data2,\n",
    "#         z_functions,\n",
    "#         m,\n",
    "#         target_accept=accept,\n",
    "#         cores=8,chains=8,\n",
    "#         keys=keys,\n",
    "#         init=\"adapt_full\",\n",
    "#         tune=samp,\n",
    "#         draws=samp)\n",
    "        \n",
    "    \n",
    "def re_weight(click):\n",
    "    \n",
    "    filtered_df_output.clear_output()\n",
    "    with filtered_df_output:\n",
    "        print('Recalculating model weights...')\n",
    "    \n",
    "    model_posterior_dir = str(model_dir)+'/'+str(f'arviz_traces_{year}')\n",
    "    model_posterior_list=[o[:-3] for o in os.listdir(model_posterior_dir) if '.nc' in o]    \n",
    "\n",
    "    all_traces = {}\n",
    "    for f in model_posterior_list:\n",
    "        all_traces[f]=az.from_netcdf(model_posterior_dir+'/'+f+'.nc')\n",
    "\n",
    "    comp = az.compare(all_traces, ic=\"loo\", method='BB-pseudo-BMA', b_samples=1000) \n",
    "    comp.to_csv(str(model_dir)+'/'+str('model_weights/model_weights.csv'))\n",
    "    \n",
    "    update_table(click)  \n",
    "    \n",
    "def weighted_inference_plot(to_run):\n",
    "    \n",
    "    model_predict_dir = str(model_dir)+'/'+str(f'pymc3_post_predict_{year}')\n",
    "    model_predict_list=[o[:-4] for o in os.listdir(model_predict_dir) if '.pkl' in o]\n",
    "    \n",
    "    preds = {}\n",
    "    for f in to_run:\n",
    "        if f in model_predict_list:\n",
    "            preds[f]=load(model_predict_dir+'/'+f+'.pkl')\n",
    "            \n",
    "    if len(preds.keys())==0:\n",
    "        print('Selected models have no traces, please run inference and generate weights..')\n",
    "        return None\n",
    "\n",
    "    model_weights = pd.read_csv(str(model_dir)+'/'+str('model_weights/model_weights.csv'),index_col=0)\n",
    "    sub_list=[m for m in to_run if m in list(model_weights.index)]\n",
    "    model_weights=model_weights.loc[sub_list]\n",
    "    if np.sum(model_weights['weight'])==0:\n",
    "        model_weights['weight']=1\n",
    "    else:\n",
    "        model_weights['weight']=model_weights['weight']/np.sum(model_weights['weight'])\n",
    "\n",
    "    gmsl=weighted_trace(preds,model_weights,iters=10000)\n",
    "\n",
    "    X_new = np.linspace(115, 130, 200)[:, np.newaxis]\n",
    "\n",
    "    f_size=18\n",
    "\n",
    "    sns.set_style(\n",
    "        \"ticks\",\n",
    "        {\n",
    "            \"axes.edgecolor\": \".3\",\n",
    "            \"xtick.color\": \".3\",\n",
    "            \"ytick.color\": \".3\",\n",
    "            \"text.color\": \".3\",\n",
    "            \"axes.facecolor\": \"(.98,.98,.98)\",\n",
    "            \"axes.grid\": True,\n",
    "            \"grid.color\": \".95\",\n",
    "            \"grid.linestyle\": u\"--\",\n",
    "        },\n",
    "    )\n",
    "    flatui = [\"#D08770\", \"#BF616A\", \"#A3BE8C\", \"#B48EAD\", \"#34495e\", \"#5E81AC\"]\n",
    "    cs = sns.color_palette(flatui)\n",
    "\n",
    "    ##Figure\n",
    "\n",
    "    scale=1.5\n",
    "    fig = plt.figure(figsize=(11*scale,4*scale))\n",
    "    ax1=fig.add_subplot()\n",
    "\n",
    "    plot_gmsl_inference(X_new,gmsl,cs[4],ax1,gmsl[1])\n",
    "    plt.gca().set_title(\n",
    "        \"A. Last Interglacial GMSL\",\n",
    "        fontsize=f_size,\n",
    "    )\n",
    "    ax1.set_aspect(1/6)\n",
    "    ax1.set_ylim([0, 12])\n",
    "    # ax1.set_yticks([-2,0,2,4,6])\n",
    "    # ax1.set_yticklabels([-2,0,2,4,6],fontsize=f_size)\n",
    "    ax1.set_xlim(117, 128)\n",
    "    ax1.invert_xaxis()\n",
    "    ax1.set_xticks(np.arange(128,116,-1))\n",
    "    ax1.set_xticklabels(np.arange(128,116,-1),fontsize=f_size)\n",
    "    ax1.legend(loc=\"best\", frameon=True, fontsize=f_size*.66)\n",
    "\n",
    "    ax1.set_ylabel(\"Global Mean Sea Level\\n(m above MSL)\", fontsize=f_size)\n",
    "    ax1.set_xlabel(\"Age (kya)\",fontsize=f_size)\n",
    "    ax1.grid(linewidth=1)\n",
    "\n",
    "\n",
    "    fig.tight_layout(w_pad=0,h_pad=0)\n",
    "    return fig\n",
    "    \n",
    "def weighted_trace(pred_list, comp, var=\"f_pred\", iters=20000):\n",
    "    weighted_trace =[]\n",
    "    \n",
    "    for i in range(iters):\n",
    "        choice = np.random.choice(np.arange(len(comp)), 1, p=comp['weight'])\n",
    "        key=comp.index[choice][0]\n",
    "        f_preds = pred_list[key][var]\n",
    "        C=np.random.choice(np.arange(len(f_preds)), 1)\n",
    "        gmsl=f_preds[C].ravel()\n",
    "        weighted_trace.append(gmsl)\n",
    "        \n",
    "    weighted_trace=np.array(weighted_trace)\n",
    "\n",
    "    return weighted_trace\n",
    "\n",
    "def plot_gmsl_inference(X_new,inference,color,ax,max_like):\n",
    "    \n",
    "    bot = np.nanpercentile(inference, 2.5, axis=0)\n",
    "    top = np.nanpercentile(inference, 97.5, axis=0)\n",
    "\n",
    "    max_like = gaussian(max_like,3)\n",
    "\n",
    "    ax.fill_between(\n",
    "            X_new.ravel(),\n",
    "            bot,\n",
    "            top,\n",
    "            fc=(1,1,1),\n",
    "            zorder=2,\n",
    "            alpha=1,\n",
    "            lw=0,\n",
    "            ec=color,\n",
    "            aa=True,\n",
    "            capstyle=\"round\",\n",
    "        )\n",
    "    ax.fill_between(\n",
    "            X_new.ravel(),\n",
    "            bot,\n",
    "            top,\n",
    "            fc=color,\n",
    "            zorder=3,\n",
    "            alpha=.1,\n",
    "            lw=0,\n",
    "            ec=color,\n",
    "            aa=True,\n",
    "            capstyle=\"round\",\n",
    "        )\n",
    "    ax.fill_between(\n",
    "        X_new.ravel(),\n",
    "        bot,\n",
    "        top,\n",
    "        fc='none',\n",
    "        zorder=4,\n",
    "        alpha=1,\n",
    "        lw=1.5,linestyle='--',\n",
    "        ec=color,\n",
    "        aa=True,\n",
    "        capstyle=\"round\",#hatch=''\n",
    "    )\n",
    "    \n",
    "    bot = np.nanpercentile(inference, 33/2, axis=0)\n",
    "    top = np.nanpercentile(inference, 100-33/2, axis=0)\n",
    "    \n",
    "    ax.fill_between(\n",
    "        X_new.ravel(),\n",
    "        bot,\n",
    "        top,\n",
    "        fc=color,\n",
    "        zorder=3,\n",
    "        alpha=.1,\n",
    "        lw=0,\n",
    "        ec=color,\n",
    "        aa=True,\n",
    "        capstyle=\"round\",\n",
    "    )\n",
    "    \n",
    "    ax.fill_between(\n",
    "        X_new.ravel(),\n",
    "        bot,\n",
    "        top,\n",
    "        fc='none',\n",
    "        zorder=4,\n",
    "        alpha=1,\n",
    "        lw=1.5,linestyle='-',\n",
    "        ec=color,\n",
    "        aa=True,\n",
    "        capstyle=\"round\",#hatch=''\n",
    "    )\n",
    "    \n",
    "    ## make legend here\n",
    "    ax.plot([],[],color=color,lw=1.5,linestyle='--',label='95% GMSL envelope')\n",
    "    ax.plot([],[],color=color,lw=1.5,linestyle='-',label='66% GMSL envelope')\n",
    "    ax.plot([],[],color=color,lw=4,label='Most likely GMSL')\n",
    "    \n",
    "\n",
    "    \n",
    "#     lig_only=((X_new<128) & (X_new>117)).ravel()\n",
    "#     ax.plot(X_new,max_like,\n",
    "#              zorder=13,color=color,lw=4)\n",
    "    \n",
    "\n",
    "    return ax\n",
    "\n",
    "def load_synth_data():\n",
    "    true_model = \"output_new71Cp420_L6G_Wael_T\" ## we'll generate synthetic data from this GIA model\n",
    "    \n",
    "    ## Synthetic GMSL\n",
    "\n",
    "    lig_start = 128\n",
    "    lig_end = 117\n",
    "    lig_age = np.linspace(lig_start,lig_end,100)\n",
    "    lig_dt = lig_age-lig_end\n",
    "    lig_synth_gmsl = 3 * np.sin(lig_dt / (0.5 * np.pi)) + 3 \n",
    "    synth_gmsl_function = interp1d(lig_age, lig_synth_gmsl,bounds_error=False,fill_value=(lig_synth_gmsl[0],lig_synth_gmsl[-1]))\n",
    "\n",
    "    ## replace real data elevations with RSL (from selected 'true' GIA model) + GMSL (from curve above)\n",
    "    data=load_data()\n",
    "    GIA_MODEL, age, model_dims = load_model(true_model)\n",
    "    within_lig = np.array((np.array(age)<=lig_start) & (np.array(age)>=lig_end))\n",
    "    age=[a for a,check in zip(age,within_lig) if check]\n",
    "    GIA_MODEL=[g for g,check in zip(GIA_MODEL,within_lig) if check]\n",
    "\n",
    "    z_functions = interpolation_functions(data[\"lat\"], data[\"lon\"], GIA_MODEL, age, model_dims)\n",
    "    \n",
    "    Es = []\n",
    "    for i in range(len(z_functions)):\n",
    "        Es.append(z_functions[i](data[\"age\"][i]))\n",
    "    data[\"rsl\"] = np.array(Es)\n",
    "    data[\"elevation\"] = np.copy(data[\"rsl\"])\n",
    "    \n",
    "    for i in range(data[\"lat\"].size):\n",
    "            data[\"elevation\"][i] += synth_gmsl_function(data[\"age\"][i])\n",
    "        \n",
    "    #adjust coral data for mean water depth 117.14214214214215\n",
    "    for i in range(data[\"lat\"].size):\n",
    "        if data['type'][i] == 'coral':\n",
    "            data[\"elevation\"][i] -= data['water depth mean (m)'][i]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def clear_and_run(click):\n",
    "    samp=samples_slider.value\n",
    "    accept=accept_slider.value\n",
    "    output_simulation.clear_output()\n",
    "    with output_simulation:     \n",
    "        to_run = list(models_df.query(\n",
    "                    f\"posterior_trace == {list(widge_post.value)} & posterior_predict == {list(widge_predict.value)} & Lithosphere == {list(widge_lith.value)} & UMV == {list(widge_umv.value)} & LMV == {list(widge_lmv.value)} & `ice_history` == {list(widge_ice.value)} & esl_curve == {list(widge_gmsl.value)}\"\n",
    "                ).index)\n",
    "    \n",
    "        backup(to_run)\n",
    "        run_inferences(to_run,samp,accept)\n",
    "        \n",
    "def plot_on_click(click):\n",
    "    models_df = get_model_status(inference_dir,model_dir/'output_glac_w_ice6g/')\n",
    "    output_simulation.clear_output(wait=True)\n",
    "    with output_simulation: \n",
    "        filtdb=models_df.query(\n",
    "                    f\"posterior_trace == {list(widge_post.value)} & posterior_predict == {list(widge_predict.value)} & Lithosphere == {list(widge_lith.value)} & UMV == {list(widge_umv.value)} & LMV == {list(widge_lmv.value)} & `ice_history` == {list(widge_ice.value)} & esl_curve == {list(widge_gmsl.value)}\")\n",
    "                \n",
    "        to_run = list(filtdb.index)\n",
    "        \n",
    "        if any(filtdb['posterior_trace']==False):\n",
    "            print('Selected models have no traces, please run inference and generate weights..')\n",
    "            return None\n",
    "            \n",
    "        fig=weighted_inference_plot(to_run)\n",
    "        plt.show()\n",
    "    return\n",
    "    \n",
    "def backup(to_run):\n",
    "    for f in to_run:\n",
    "        try:\n",
    "            model_posterior_dir = str(inference_dir)+'/'+str(f'arviz_traces_{year}')\n",
    "            os.rename(model_posterior_dir+'/'+f+'.nc', \n",
    "                      model_posterior_dir+'_backup/'+f+'.nc')\n",
    "            model_predict_dir = str(inference_dir)+'/'+str(f'pymc3_post_predict_{year}')\n",
    "            os.rename(model_predict_dir+'/'+f+'.pkl', \n",
    "                      model_predict_dir+'_backup/'+f+'.pkl')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "def run_inferences(to_run,samp,accept):\n",
    "\n",
    "    data2 = load_data()\n",
    "    N = data2[\"elevation\"].size\n",
    "    data2['age_uncertainty']=data2['age_uncertainty']\n",
    "    data2=data2.sort_values(['type'])\n",
    "    data2 = data2[data2['type']!='limiting']\n",
    "    keys = list(data2['type'].unique())\n",
    "    \n",
    "    count=1\n",
    "\n",
    "    for m in to_run:\n",
    "        clear_output(wait=True)\n",
    "        print(\"running simulation number \" + str(count) + \" of \" + str(len(to_run)))\n",
    "        print(\"running model: \" + m)\n",
    "        count+=1\n",
    "        model_name = m\n",
    "\n",
    "        ## Build the statistical model\n",
    "        GIA_MODEL, age, model_dims = load_model(m)\n",
    "        z_functions = interpolation_functions(data2[\"lat\"], data2[\"lon\"], GIA_MODEL, age, model_dims)\n",
    "        model, gp = inference_model_new(data2,z_functions,keys=keys)\n",
    "        \n",
    "        with model:\n",
    "            ## The Hamiltonian Monte-Carlo sampling step, ie the inference button\n",
    "            az_trace = pm.sample(tune=samp,draws=samp,target_accept=accept,\n",
    "                    init='adapt_full', progressbar=True, cores=8, chains=8, \n",
    "                              return_inferencedata=True\n",
    "                )\n",
    "\n",
    "            #### After fitting, lets make predictions for GMSL at the ages in X_new\n",
    "            X_new = np.linspace(115, 130, 200)[:, np.newaxis]\n",
    "            f_pred = gp.conditional(\n",
    "                \"f_pred\", X_new, pred_noise=False\n",
    "            )  \n",
    "\n",
    "            ## and we will collect our hard work in this subfolders\n",
    "            model_posterior_dir = str(inference_dir)+'/'+str(f'arviz_traces_{year}/')\n",
    "            model_predict_dir = str(inference_dir)+'/'+str(f'pymc3_post_predict_{year}/')\n",
    "\n",
    "            ## A hacky try/except structure to keep a loop of inferences running over night if\n",
    "            ## something breaks on a given run\n",
    "            try:\n",
    "                pred_samples = pm.fast_sample_posterior_predictive(az_trace, var_names=['f_pred'])\n",
    "\n",
    "                ## Export arviz trace object to netcdf\n",
    "                az_trace.to_netcdf(model_posterior_dir+model_name+'.nc',groups=[\"posterior\",\"log_likelihood\",\"sample_stats\"])\n",
    "\n",
    "                ## Export prediction dictionary to python pickle (dict with arrays)\n",
    "                with open(model_predict_dir+model_name+'.pkl', \"wb\") as buff:\n",
    "                    pickle.dump(pred_samples, buff)\n",
    "                print('Success')\n",
    "                time.sleep(1)\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                print('Sampling failed, no output saved')\n",
    "                time.sleep(1)\n",
    "        \n",
    "def re_weight(click):\n",
    "    \n",
    "    filtered_df_output.clear_output()\n",
    "    with filtered_df_output:\n",
    "        print('Recalculating model weights...')\n",
    "    \n",
    "    model_posterior_dir = str(inference_dir)+'/'+str(f'arviz_traces_{year}')\n",
    "    model_posterior_list=[o[:-3] for o in os.listdir(model_posterior_dir) if '.nc' in o]    \n",
    "\n",
    "    all_traces = {}\n",
    "    for f in model_posterior_list:\n",
    "        all_traces[f]=az.from_netcdf(model_posterior_dir+'/'+f+'.nc')\n",
    "\n",
    "    comp = az.compare(all_traces, ic=\"loo\", method='BB-pseudo-BMA', b_samples=50000, alpha=1) \n",
    "    comp.to_csv(str(inference_dir)+'/'+str('model_weights/model_weights.csv'))\n",
    "    \n",
    "    update_table(click)\n",
    "\n",
    "def weighted_inference_plot(to_run):\n",
    "    \n",
    "    if type(to_run)!=type([]):\n",
    "        to_run=[to_run]\n",
    "    \n",
    "    model_predict_dir = str(inference_dir)+'/'+str(f'pymc3_post_predict_{year}')\n",
    "    model_predict_list=[o[:-4] for o in os.listdir(model_predict_dir) if '.pkl' in o]\n",
    "    \n",
    "    preds = {}\n",
    "    for f in to_run:\n",
    "        if f in model_predict_list:\n",
    "            preds[f]=load(model_predict_dir+'/'+f+'.pkl')\n",
    "            \n",
    "    if len(preds.keys())==0:\n",
    "        print('Selected models have no traces, please run inference and generate weights..')\n",
    "        return None\n",
    "\n",
    "    model_weights = pd.read_csv(str(inference_dir)+'/'+str('model_weights/model_weights.csv'),index_col=0)\n",
    "    sub_list=[m for m in to_run if m in list(model_weights.index)]\n",
    "    model_weights=model_weights.loc[sub_list]\n",
    "    if np.sum(model_weights['weight'])==0:\n",
    "        model_weights['weight']=1\n",
    "    else:\n",
    "        model_weights['weight']=model_weights['weight']/np.sum(model_weights['weight'])\n",
    "\n",
    "    gmsl=weighted_trace(preds,model_weights,iters=10000)\n",
    "\n",
    "    X_new = np.linspace(115, 130, 200)[:, np.newaxis]\n",
    "\n",
    "    f_size=18\n",
    "\n",
    "    sns.set_style(\n",
    "        \"ticks\",\n",
    "        {\n",
    "            \"axes.edgecolor\": \".3\",\n",
    "            \"xtick.color\": \".3\",\n",
    "            \"ytick.color\": \".3\",\n",
    "            \"text.color\": \".3\",\n",
    "            \"axes.facecolor\": \"(.98,.98,.98)\",\n",
    "            \"axes.grid\": True,\n",
    "            \"grid.color\": \".95\",\n",
    "            \"grid.linestyle\": u\"--\",\n",
    "        },\n",
    "    )\n",
    "    flatui = [\"#D08770\", \"#BF616A\", \"#A3BE8C\", \"#B48EAD\", \"#34495e\", \"#5E81AC\"]\n",
    "    cs = sns.color_palette(flatui)\n",
    "\n",
    "    ##Figure\n",
    "\n",
    "    scale=1.5\n",
    "    fig = plt.figure(figsize=(11*scale,4*scale))\n",
    "    ax1=fig.add_subplot()\n",
    "\n",
    "    plot_gmsl_inference(X_new,gmsl,cs[4],ax1,False)\n",
    "    plt.gca().set_title(\n",
    "        \"A. Last Interglacial GMSL\",\n",
    "        fontsize=f_size,\n",
    "    )\n",
    "    ax1.set_aspect(1/2)\n",
    "    ax1.set_ylim([0, 12])\n",
    "    # ax1.set_yticks([-2,0,2,4,6])\n",
    "    # ax1.set_yticklabels([-2,0,2,4,6],fontsize=f_size)\n",
    "    ax1.set_xlim(117, 128)\n",
    "    ax1.invert_xaxis()\n",
    "    ax1.set_xticks(np.arange(128,116,-1))\n",
    "    ax1.set_xticklabels(np.arange(128,116,-1),fontsize=f_size)\n",
    "    ax1.legend(loc=\"best\", frameon=True, fontsize=f_size*.66)\n",
    "\n",
    "    ax1.set_ylabel(\"Global Mean Sea Level\\n(m above MSL)\", fontsize=f_size)\n",
    "    ax1.set_xlabel(\"Age (kya)\",fontsize=f_size)\n",
    "    ax1.grid(linewidth=1)\n",
    "    \n",
    "#     ## Synthetic GMSL\n",
    "#     subsidence = 2.5\n",
    "#     lig_start = 128\n",
    "#     lig_end = 117\n",
    "#     lig_age = np.linspace(lig_start,lig_end,100)\n",
    "#     lig_dt = lig_age-lig_end\n",
    "#     lig_synth_gmsl = 3 * np.sin(lig_dt / (0.5 * np.pi)) + 3 + subsidence\n",
    "    \n",
    "#     ax1.plot(lig_age, lig_synth_gmsl, label=\"Simulation GMSL\", lw=3, linestyle='--', zorder=30, color='k')\n",
    "\n",
    "\n",
    "    fig.tight_layout(w_pad=0,h_pad=0)\n",
    "    return fig\n",
    "\n",
    "models_df = get_model_status(inference_dir,model_dir/'output_glac_w_ice6g/')\n",
    "fmt = Layout(width=\"5vw\", height=\"250px\")\n",
    "filtered_df_output = widgets.Output()\n",
    "output_simulation = widgets.Output()\n",
    "\n",
    "\n",
    "def update_table(change):\n",
    "    models_df = get_model_status(inference_dir,model_dir/'output_glac_w_ice6g/')\n",
    "    filtered_df_output.clear_output(wait=True)\n",
    "    with filtered_df_output:\n",
    "        display(\n",
    "            models_df.query(\n",
    "                f\"posterior_trace == {list(widge_post.value)} & posterior_predict == {list(widge_predict.value)} & Lithosphere == {list(widge_lith.value)} & UMV == {list(widge_umv.value)} & LMV == {list(widge_lmv.value)} & `ice_history` == {list(widge_ice.value)} & esl_curve == {list(widge_gmsl.value)}\"\n",
    "            ).sort_values('weight',ascending=False)\n",
    "        )\n",
    "\n",
    "\n",
    "widge_lith = widgets.SelectMultiple(\n",
    "    options=np.sort(models_df[\"Lithosphere\"].unique().astype(int)),\n",
    "    # rows=10,\n",
    "    #     description=\"Lithosphere\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "\n",
    "widge_umv = widgets.SelectMultiple(\n",
    "    options=np.sort(models_df[\"UMV\"].unique().astype(int)),\n",
    "    # rows=10,\n",
    "    #     description=\"UMV\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "widge_lmv = widgets.SelectMultiple(\n",
    "    options=np.sort(models_df[\"LMV\"].unique().astype(int)),\n",
    "    # rows=10,\n",
    "    #     description=\"LMV\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "widge_ice = widgets.SelectMultiple(\n",
    "    options=models_df[\"ice_history\"].unique(),\n",
    "    # rows=10,\n",
    "    #     description=\"Ice History\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "\n",
    "widge_gmsl = widgets.SelectMultiple(\n",
    "    options=models_df[\"esl_curve\"].unique(),\n",
    "    # rows=10,\n",
    "    #     description=\"GMSL\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "widge_post = widgets.SelectMultiple(\n",
    "    options=[True, False],\n",
    "    # rows=10,\n",
    "    #     description=\"Posterior Trace\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "widge_predict = widgets.SelectMultiple(\n",
    "    options=[True, False],\n",
    "    # rows=10,\n",
    "    #     description=\"Posterior Prediction\",\n",
    "    disabled=False,\n",
    "    layout=fmt,\n",
    ")\n",
    "\n",
    "samples_slider=widgets.IntSlider(value=500,\n",
    "    min=100,step=50,\n",
    "    max=2000,layout=fmt,orientation='vertical')\n",
    "\n",
    "accept_slider=widgets.FloatSlider(value=.95,\n",
    "    min=.8,step=0.01,\n",
    "    max=1,layout=fmt,orientation='vertical')\n",
    "\n",
    "label_list = [\n",
    "    \"Lithosphere\",\n",
    "    \"UMV\",\n",
    "    \"LMV\",\n",
    "    \"ice_history\",\n",
    "    \"esl_curve\",\n",
    "    \"posterior_trace\",\n",
    "    \"posterior_prediction\",\n",
    "    \"Posterior samples\",\n",
    "    \"Acceptance Target\"\n",
    "]\n",
    "widget_list = [\n",
    "    widge_lith,\n",
    "    widge_umv,\n",
    "    widge_lmv,\n",
    "    widge_ice,\n",
    "    widge_gmsl,\n",
    "    widge_post,\n",
    "    widge_predict,\n",
    "    samples_slider,\n",
    "    accept_slider\n",
    "]\n",
    "\n",
    "wv_list = [VBox([Label(l), w]) for l, w in zip(label_list, widget_list)]\n",
    "h_box = HBox(wv_list)\n",
    "\n",
    "for w in widget_list:\n",
    "    w.observe(update_table)\n",
    "\n",
    "N_button = 3\n",
    "pct = 100 / N_button\n",
    "bt_layout = Layout(width=str(int(pct)) + \"%\")\n",
    "plot_inference_button = Button(\n",
    "    description=\"Plot weighted inference for selected\", layout=bt_layout\n",
    ")\n",
    "rerun_weights = Button(description=\"Recalculate weights for all\", layout=bt_layout)\n",
    "rerun_inference_button = Button(\n",
    "    description=\"Rerun GMSL inference for selected\", layout=bt_layout\n",
    ")\n",
    "\n",
    "plot_inference_button.on_click(plot_on_click)\n",
    "rerun_weights.on_click(re_weight)\n",
    "rerun_inference_button.on_click(clear_and_run)\n",
    "\n",
    "buttons = [plot_inference_button, rerun_weights, rerun_inference_button]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bahamas_lig",
   "language": "python",
   "name": "bahamas_lig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
